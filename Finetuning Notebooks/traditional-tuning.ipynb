{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-29T06:47:09.580640Z","iopub.status.busy":"2024-10-29T06:47:09.580357Z","iopub.status.idle":"2024-10-29T06:47:24.317635Z","shell.execute_reply":"2024-10-29T06:47:24.316666Z","shell.execute_reply.started":"2024-10-29T06:47:09.580608Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6f851f5e94da4aab8d279bd508d26ded14cb27771b3f0a40ff35df6b55d5a753\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:47:24.319229Z","iopub.status.busy":"2024-10-29T06:47:24.318902Z","iopub.status.idle":"2024-10-29T06:47:30.610449Z","shell.execute_reply":"2024-10-29T06:47:30.609486Z","shell.execute_reply.started":"2024-10-29T06:47:24.319194Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import pandas as pd\n","from tqdm import tqdm\n","import time\n","from torch.nn import CrossEntropyLoss\n","from rouge_score import rouge_scorer"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:47:31.618168Z","iopub.status.busy":"2024-10-29T06:47:31.617261Z","iopub.status.idle":"2024-10-29T06:47:31.623161Z","shell.execute_reply":"2024-10-29T06:47:31.622089Z","shell.execute_reply.started":"2024-10-29T06:47:31.618129Z"},"trusted":true},"outputs":[],"source":["MODEL_NAME = \"gpt2\"\n","BATCH_SIZE = 1\n","EPOCHS = 1\n","MAX_LEN = 1024\n","GRADIENT_ACCUMULATION_STEPS = 4\n","GRADIENT_CLIP_NORM = 1.0\n","EARLY_STOPPING_PATIENCE = 1\n","LEARNING_RATE = 1e-3"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:47:31.937736Z","iopub.status.busy":"2024-10-29T06:47:31.937328Z","iopub.status.idle":"2024-10-29T06:47:31.946147Z","shell.execute_reply":"2024-10-29T06:47:31.944942Z","shell.execute_reply.started":"2024-10-29T06:47:31.937696Z"},"trusted":true},"outputs":[],"source":["def load_and_preprocess_data(file_path):\n","    df = pd.read_csv(file_path)\n","    df = df.dropna().sample(frac=0.1)  # Use only 10% of the data\n","\n","    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","\n","    tokenized_articles = []\n","    tokenized_summaries = []\n","    for article, summary in zip(df[\"article\"], df[\"highlights\"]):\n","        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n","        max_length_article = MAX_LEN\n","        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_length_article)\n","        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=MAX_LEN)\n","\n","        padded_article = article_tokens + [tokenizer.eos_token_id] * (max_length_article - len(article_tokens))\n","        padded_summary = summary_tokens + [tokenizer.eos_token_id] * (MAX_LEN - len(summary_tokens))\n","\n","        tokenized_articles.append(padded_article)\n","        tokenized_summaries.append(padded_summary)\n","\n","    return tokenized_articles, tokenized_summaries"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:47:32.428475Z","iopub.status.busy":"2024-10-29T06:47:32.427644Z","iopub.status.idle":"2024-10-29T06:52:14.392076Z","shell.execute_reply":"2024-10-29T06:52:14.391225Z","shell.execute_reply.started":"2024-10-29T06:47:32.428435Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26f4e5fc6ede46b7864891a8d1665bd4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"350ed3ab798e4a9b9d8bcdbae5cc98d6","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"260e26ecf68f425caf65edae6ea83c86","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e322f71de324961b9908daefb035c0a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"355fd58009b34c5c82450b702f4f29ac","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","tokenized_articles_train, tokenized_summaries_train = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\")\n","tokenized_articles_validation, tokenized_summaries_validation = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\")\n","tokenized_articles_test, tokenized_summaries_test = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:52:14.394210Z","iopub.status.busy":"2024-10-29T06:52:14.393862Z","iopub.status.idle":"2024-10-29T06:52:14.490489Z","shell.execute_reply":"2024-10-29T06:52:14.489445Z","shell.execute_reply.started":"2024-10-29T06:52:14.394174Z"},"trusted":true},"outputs":[],"source":["tokenized_articles_train = tokenized_articles_train[:21000]\n","tokenized_summaries_train = tokenized_summaries_train[:21000]\n","\n","tokenized_articles_validation = tokenized_articles_validation[:6000]\n","tokenized_summaries_validation = tokenized_summaries_validation[:6000]\n","\n","tokenized_articles_test = tokenized_articles_test[:3000]\n","tokenized_summaries_test = tokenized_summaries_test[:3000]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:21.451551Z","iopub.status.busy":"2024-10-29T10:07:21.450815Z","iopub.status.idle":"2024-10-29T10:07:21.900121Z","shell.execute_reply":"2024-10-29T10:07:21.899166Z","shell.execute_reply.started":"2024-10-29T10:07:21.451510Z"},"trusted":true},"outputs":[],"source":["model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:22.800685Z","iopub.status.busy":"2024-10-29T10:07:22.799855Z","iopub.status.idle":"2024-10-29T10:07:22.806011Z","shell.execute_reply":"2024-10-29T10:07:22.804969Z","shell.execute_reply.started":"2024-10-29T10:07:22.800647Z"},"trusted":true},"outputs":[],"source":["# Freeze all layers except the LM head\n","for param in model.transformer.parameters():\n","    param.requires_grad = False\n","for param in model.lm_head.parameters():\n","    param.requires_grad = True\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:24.055234Z","iopub.status.busy":"2024-10-29T10:07:24.054831Z","iopub.status.idle":"2024-10-29T10:07:24.061500Z","shell.execute_reply":"2024-10-29T10:07:24.060434Z","shell.execute_reply.started":"2024-10-29T10:07:24.055197Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of trainable parameters: 38597376\n"]}],"source":["trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Number of trainable parameters: {trainable_params}\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:27.327696Z","iopub.status.busy":"2024-10-29T10:07:27.326961Z","iopub.status.idle":"2024-10-29T10:07:27.335812Z","shell.execute_reply":"2024-10-29T10:07:27.334716Z","shell.execute_reply.started":"2024-10-29T10:07:27.327649Z"},"trusted":true},"outputs":[],"source":["def calculate_rouge(predictions, references):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = [scorer.score(pred, ref) for pred, ref in zip(predictions, references)]\n","    avg_rouge1 = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n","    avg_rouge2 = sum([s['rouge2'].fmeasure for s in scores]) / len(scores)\n","    avg_rougeL = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n","    return avg_rouge1, avg_rouge2, avg_rougeL\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:27.512323Z","iopub.status.busy":"2024-10-29T10:07:27.511757Z","iopub.status.idle":"2024-10-29T10:07:27.531835Z","shell.execute_reply":"2024-10-29T10:07:27.530958Z","shell.execute_reply.started":"2024-10-29T10:07:27.512286Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from tqdm import tqdm\n","\n","def fine_tune_on_summarization(model, train_articles, train_summaries, val_articles, val_summaries):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)\n","    best_val_loss = float('inf')\n","    no_improvement_epochs = 0\n","\n","    total_start_time = time.time()\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        train_loss = 0\n","        train_start_event = torch.cuda.Event(enable_timing=True)\n","        train_end_event = torch.cuda.Event(enable_timing=True)\n","        train_start_event.record()\n","\n","        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n","            for idx, (article, summary) in progress:\n","                input_ids = torch.tensor(article).to(device)\n","                labels = torch.tensor(summary).to(device)\n","                outputs = model(input_ids=input_ids)\n","                logits = outputs.logits\n","\n","                loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","                train_loss += loss.item()\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            avg_train_loss = train_loss / len(train_articles)\n","            print(f\"Train Loss (Epoch {epoch + 1}): {avg_train_loss:.4f}\")\n","\n","        train_end_event.record()\n","        train_end_event.synchronize()\n","        train_gpu_time = train_start_event.elapsed_time(train_end_event)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_predictions = []\n","        val_references = []\n","        val_start_event = torch.cuda.Event(enable_timing=True)\n","        val_end_event = torch.cuda.Event(enable_timing=True)\n","        val_start_event.record()\n","\n","        with torch.no_grad():\n","            for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc=\"Validation\", unit=\"batch\"):\n","                input_ids = torch.tensor(article).to(device)\n","                labels = torch.tensor(summary).to(device)\n","                outputs = model(input_ids=input_ids)\n","                logits = outputs.logits\n","\n","                loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","                val_loss += loss.item()\n","\n","                predicted_token_ids = torch.argmax(logits, dim=-1)\n","                pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","                ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","                val_predictions.append(pred_text)\n","                val_references.append(ref_text)\n","\n","            avg_val_loss = val_loss / len(val_articles)\n","            avg_rouge1, avg_rouge2, avg_rougeL = calculate_rouge(val_predictions, val_references)\n","            print(f\"Val Loss (Epoch {epoch + 1}): {avg_val_loss:.4f}\")\n","            print(f\"Val ROUGE-1: {avg_rouge1:.4f}, Val ROUGE-2: {avg_rouge2:.4f}, Val ROUGE-L: {avg_rougeL:.4f}\")\n","\n","        val_end_event.record()\n","        val_end_event.synchronize()\n","        val_gpu_time = val_start_event.elapsed_time(val_end_event)\n","\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            no_improvement_epochs = 0\n","        else:\n","            no_improvement_epochs += 1\n","            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n","                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n","                break\n","\n","        print(f\"GPU Compute Time (Train Epoch {epoch + 1}): {train_gpu_time:.2f} ms\")\n","        print(f\"GPU Compute Time (Validation Epoch {epoch + 1}): {val_gpu_time:.2f} ms\")\n","        \n","        torch.save(model.state_dict(), f\"traditional_finetuning_{epoch}.pth\")\n","\n","    total_end_time = time.time()\n","    total_training_time = total_end_time - total_start_time\n","    print(f\"Total training time: {total_training_time:.2f} seconds\")\n","\n","    return model\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:07:29.403257Z","iopub.status.busy":"2024-10-29T10:07:29.402897Z","iopub.status.idle":"2024-10-29T11:40:49.161543Z","shell.execute_reply":"2024-10-29T11:40:49.160122Z","shell.execute_reply.started":"2024-10-29T10:07:29.403224Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1: 100%|██████████| 21000/21000 [1:29:28<00:00,  3.91batch/s]\n","Train Loss (Epoch 1): 8.2446\n","Validation: 100%|██████████| 1337/1337 [02:35<00:00,  8.62batch/s]\n","Val Loss (Epoch 1): 7.6454\n","Val ROUGE-1: 0.0791, Val ROUGE-2: 0.0040, Val ROUGE-L: 0.0575\n","GPU Compute Time (Train Epoch 1): 5369129.50 ms\n","GPU Compute Time (Validation Epoch 1): 183794.66 ms\n","Total training time: 5368 seconds\n"]}],"source":["fine_tuned_model = fine_tune_on_summarization(model, tokenized_articles_train, tokenized_summaries_train, tokenized_articles_validation, tokenized_summaries_validation)\n","\n","torch.save(fine_tuned_model.state_dict(), 'traditional_finetuning.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:40:49.162327Z","iopub.status.idle":"2024-10-29T11:40:49.162707Z","shell.execute_reply":"2024-10-29T11:40:49.162528Z","shell.execute_reply.started":"2024-10-29T11:40:49.162510Z"},"trusted":true},"outputs":[],"source":["# Evaluate the fine-tuned model\n","fine_tuned_model.eval()\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)\n","test_predictions = []\n","test_references = []\n","\n","with torch.no_grad():\n","    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Testing\", unit=\"batch\"):\n","        input_ids = torch.tensor(article).to(device)\n","        labels = torch.tensor(summary).to(device)\n","\n","        # Forward pass to get logits\n","        outputs = fine_tuned_model(input_ids=input_ids)\n","        logits = outputs.logits\n","\n","        # Calculate loss using criterion\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","        test_loss += loss.item()\n","\n","        # Get predictions\n","        predicted_token_ids = torch.argmax(logits, dim=-1)\n","        pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","        ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","        test_predictions.append(pred_text)\n","        test_references.append(ref_text)\n","\n","    # Calculate test ROUGE scores\n","    avg_rouge1_test, avg_rouge2_test, avg_rougeL_test = calculate_rouge(test_predictions, test_references)\n","    print(f\"Test ROUGE-1: {avg_rouge1_test:.4f}, Test ROUGE-2: {avg_rouge2_test:.4f}, Test ROUGE-L: {avg_rougeL_test:.4f}\")\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T11:41:10.736493Z","iopub.status.busy":"2024-10-29T11:41:10.736110Z","iopub.status.idle":"2024-10-29T11:41:11.635050Z","shell.execute_reply":"2024-10-29T11:41:11.634110Z","shell.execute_reply.started":"2024-10-29T11:41:10.736456Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/4097015524.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/kaggle/working/traditional_finetuning_0.pth'))\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n","\n","# Load the fine-tuned weights\n","model.load_state_dict(torch.load('traditional_finetuning.pth'))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T11:41:22.346903Z","iopub.status.busy":"2024-10-29T11:41:22.346534Z","iopub.status.idle":"2024-10-29T11:43:59.555365Z","shell.execute_reply":"2024-10-29T11:43:59.554252Z","shell.execute_reply.started":"2024-10-29T11:41:22.346867Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing: 100%|██████████| 1149/1149 [02:13<00:00,  8.59batch/s]\n","Test ROUGE-1: 0.0796, Test ROUGE-2: 0.0041, Test ROUGE-L: 0.0579\n"]}],"source":["# Evaluate the fine-tuned model\n","model.eval()\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)\n","test_predictions = []\n","test_references = []\n","test_loss=0.0\n","\n","with torch.no_grad():\n","    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Testing\", unit=\"batch\"):\n","        input_ids = torch.tensor(article).to(device)\n","        labels = torch.tensor(summary).to(device)\n","\n","        # Forward pass to get logits\n","        outputs = model(input_ids=input_ids)\n","        logits = outputs.logits\n","\n","        # Calculate loss using criterion\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","        test_loss += loss.item()\n","\n","        # Get predictions\n","        predicted_token_ids = torch.argmax(logits, dim=-1)\n","        pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","        ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","        test_predictions.append(pred_text)\n","        test_references.append(ref_text)\n","\n","    # Calculate test ROUGE scores\n","    avg_rouge1_test, avg_rouge2_test, avg_rougeL_test = calculate_rouge(test_predictions, test_references)\n","    print(f\"Test ROUGE-1: {avg_rouge1_test:.4f}, Test ROUGE-2: {avg_rouge2_test:.4f}, Test ROUGE-L: {avg_rougeL_test:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1654566,"sourceId":2734496,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
