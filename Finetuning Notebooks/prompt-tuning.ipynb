{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:18.235939Z","iopub.status.busy":"2024-10-29T10:49:18.235634Z","iopub.status.idle":"2024-10-29T10:49:33.287818Z","shell.execute_reply":"2024-10-29T10:49:33.286648Z","shell.execute_reply.started":"2024-10-29T10:49:18.235906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=fbf7545358816c1b36a4f8259a26a7ebd9baa3c386cd954e9b569b8fdec97f7a\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"markdown","metadata":{},"source":["## Import necessary libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:33.290262Z","iopub.status.busy":"2024-10-29T10:49:33.289944Z","iopub.status.idle":"2024-10-29T10:49:38.678793Z","shell.execute_reply":"2024-10-29T10:49:38.678018Z","shell.execute_reply.started":"2024-10-29T10:49:33.290227Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.680286Z","iopub.status.busy":"2024-10-29T10:49:38.679869Z","iopub.status.idle":"2024-10-29T10:49:38.684885Z","shell.execute_reply":"2024-10-29T10:49:38.683938Z","shell.execute_reply.started":"2024-10-29T10:49:38.680252Z"},"trusted":true},"outputs":[],"source":["# Constants\n","MODEL_NAME = \"gpt2\"\n","BATCH_SIZE = 1\n","EPOCHS = 1\n","PROMPT_TOKEN = \"[SUMMARIZE]\"\n","MAX_LEN = 1024\n","LEARNING_RATE = 1e-4"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.687996Z","iopub.status.busy":"2024-10-29T10:49:38.687716Z","iopub.status.idle":"2024-10-29T10:49:38.711369Z","shell.execute_reply":"2024-10-29T10:49:38.710435Z","shell.execute_reply.started":"2024-10-29T10:49:38.687966Z"},"trusted":true},"outputs":[],"source":["# Soft Prompt Vocabulary\n","soft_prompt_vocab = [\"[SUMMARIZE]\"]  # Define your custom vocabulary here\n","\n","# Create a word2idx dictionary for the soft prompt vocabulary\n","soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n","\n","num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n","prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.712720Z","iopub.status.busy":"2024-10-29T10:49:38.712383Z","iopub.status.idle":"2024-10-29T10:49:38.717587Z","shell.execute_reply":"2024-10-29T10:49:38.716511Z","shell.execute_reply.started":"2024-10-29T10:49:38.712689Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(num_prompts)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading Soft Prompt GPT2 model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.718902Z","iopub.status.busy":"2024-10-29T10:49:38.718638Z","iopub.status.idle":"2024-10-29T10:49:38.728194Z","shell.execute_reply":"2024-10-29T10:49:38.727396Z","shell.execute_reply.started":"2024-10-29T10:49:38.718873Z"},"trusted":true},"outputs":[],"source":["# Model Architecture\n","class GPT2WithSoftPrompt(torch.nn.Module):\n","    def __init__(self, model_name, num_prompts, embedding_size=768):\n","        super().__init__()\n","        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n","        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n","\n","    def forward(self, input_ids, prompt_ids):\n","        prompt_embeddings = self.soft_prompt(prompt_ids)\n","        base_embeddings = self.gpt2.transformer.wte(input_ids)\n","        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n","        outputs = self.gpt2(inputs_embeds=embeddings)\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess Data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.729712Z","iopub.status.busy":"2024-10-29T10:49:38.729264Z","iopub.status.idle":"2024-10-29T10:49:38.738337Z","shell.execute_reply":"2024-10-29T10:49:38.737413Z","shell.execute_reply.started":"2024-10-29T10:49:38.729671Z"},"trusted":true},"outputs":[],"source":["# Data Loading and Preprocessing\n","def load_and_preprocess_data(file_path, num_prompts):\n","    df = pd.read_csv(file_path)\n","    df = df.dropna().sample(frac=0.1)  # Use only 10% of the data\n","\n","    # Perform preprocessing on the data\n","    tokenized_articles = []\n","    tokenized_summaries = []\n","\n","    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","\n","    for article, summary in zip(df[\"article\"], df[\"highlights\"]):\n","        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n","        max_length_article = MAX_LEN - num_prompts\n","        article_tokens = tokenizer.encode(article, truncation=True, max_length=max_length_article)\n","        summary_tokens = tokenizer.encode(summary, truncation=True, max_length=300)\n","\n","        max_length_summary = MAX_LEN\n","        padded_article = article_tokens + [tokenizer.eos_token_id] * (max_length_article - len(article_tokens))\n","        padded_summary = summary_tokens + [tokenizer.eos_token_id] * (max_length_summary - len(summary_tokens))\n","\n","        tokenized_articles.append(padded_article)\n","        tokenized_summaries.append(padded_summary)\n","\n","\n","    return tokenized_articles, tokenized_summaries"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:49:38.739628Z","iopub.status.busy":"2024-10-29T10:49:38.739317Z","iopub.status.idle":"2024-10-29T10:54:24.396825Z","shell.execute_reply":"2024-10-29T10:54:24.395788Z","shell.execute_reply.started":"2024-10-29T10:49:38.739597Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e60a7ffb73ed4667ab65ef7b6d7f61a6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"721f55573b58452c80c0f133dbc49109","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02a2c345381d4b289f0aee17e4370ad5","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f36cff9629a3432f918ad73871589c5a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8abb0eccbe964e9989845a70809bdecb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Load and preprocess the data\n","tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","\n","tokenized_articles_train, tokenized_summaries_train = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\", num_prompts)\n","tokenized_articles_validation, tokenized_summaries_validation = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\", num_prompts)\n","tokenized_articles_test, tokenized_summaries_test = load_and_preprocess_data(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\", num_prompts)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:54:34.689388Z","iopub.status.busy":"2024-10-29T10:54:34.688998Z","iopub.status.idle":"2024-10-29T10:54:34.696746Z","shell.execute_reply":"2024-10-29T10:54:34.695807Z","shell.execute_reply.started":"2024-10-29T10:54:34.689337Z"},"trusted":true},"outputs":[],"source":["tokenized_articles_train = tokenized_articles_train[:21000]\n","tokenized_summaries_train = tokenized_summaries_train[:21000]\n","\n","tokenized_articles_validation = tokenized_articles_validation[:6000]\n","tokenized_summaries_validation = tokenized_summaries_validation[:6000]\n","\n","tokenized_articles_test = tokenized_articles_test[:3000]\n","tokenized_summaries_test = tokenized_summaries_test[:3000]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:54:34.893092Z","iopub.status.busy":"2024-10-29T10:54:34.891915Z","iopub.status.idle":"2024-10-29T10:54:35.634981Z","shell.execute_reply":"2024-10-29T10:54:35.633958Z","shell.execute_reply.started":"2024-10-29T10:54:34.893045Z"},"trusted":true},"outputs":[],"source":["model_vanilla = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:54:35.640148Z","iopub.status.busy":"2024-10-29T10:54:35.639230Z","iopub.status.idle":"2024-10-29T10:54:36.530683Z","shell.execute_reply":"2024-10-29T10:54:36.529585Z","shell.execute_reply.started":"2024-10-29T10:54:35.640102Z"},"trusted":true},"outputs":[],"source":["# # Model Initialization\n","model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:54:39.140239Z","iopub.status.busy":"2024-10-29T10:54:39.139508Z","iopub.status.idle":"2024-10-29T10:54:39.145366Z","shell.execute_reply":"2024-10-29T10:54:39.144398Z","shell.execute_reply.started":"2024-10-29T10:54:39.140202Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","BATCH_SIZE = 1\n","EPOCHS = 1\n","GRADIENT_ACCUMULATION_STEPS = 4\n","GRADIENT_CLIP_NORM = 1.0\n","EARLY_STOPPING_PATIENCE = 1\n","prompt_id = prompt_id.to(device)\n","\n","# Import cross_entropy_loss\n","from torch.nn import CrossEntropyLoss"]},{"cell_type":"markdown","metadata":{},"source":["## Rouge Score Calculation"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:57:24.205304Z","iopub.status.busy":"2024-10-29T10:57:24.204550Z","iopub.status.idle":"2024-10-29T10:57:24.212335Z","shell.execute_reply":"2024-10-29T10:57:24.211398Z","shell.execute_reply.started":"2024-10-29T10:57:24.205263Z"},"trusted":true},"outputs":[],"source":["def calculate_rouge(predictions, references):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = [scorer.score(pred, ref) for pred, ref in zip(predictions, references)]\n","    avg_rouge1 = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n","    avg_rouge2 = sum([s['rouge2'].fmeasure for s in scores]) / len(scores)\n","    avg_rougeL = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n","    return avg_rouge1, avg_rouge2, avg_rougeL"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:52:11.227026Z","iopub.status.busy":"2024-10-29T06:52:11.226696Z","iopub.status.idle":"2024-10-29T06:52:11.236061Z","shell.execute_reply":"2024-10-29T06:52:11.235199Z","shell.execute_reply.started":"2024-10-29T06:52:11.226987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params:  768\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","old_params = count_parameters(model_vanilla)\n","new_params = count_parameters(model)\n","added_params = new_params - old_params\n","print(\"Trainable params: \", added_params)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:52:11.238061Z","iopub.status.busy":"2024-10-29T06:52:11.237496Z","iopub.status.idle":"2024-10-29T06:52:12.693015Z","shell.execute_reply":"2024-10-29T06:52:12.692234Z","shell.execute_reply.started":"2024-10-29T06:52:11.238018Z"},"trusted":true},"outputs":[],"source":["import time\n","import torch\n","from tqdm import tqdm\n","from torch.nn import CrossEntropyLoss\n","from rouge_score import rouge_scorer\n","\n","def fine_tune_on_summarization(model, train_articles, train_summaries, val_articles, val_summaries):\n","    optimizer = torch.optim.Adam(model.soft_prompt.parameters(), LEARNING_RATE)\n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.eos_token_id)\n","    best_val_loss = float('inf')\n","    no_improvement_epochs = 0\n","\n","    total_start_time = time.time()  # Start measuring total training time\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        train_loss = 0\n","\n","        # GPU time measurement\n","        train_start_event = torch.cuda.Event(enable_timing=True)\n","        train_end_event = torch.cuda.Event(enable_timing=True)\n","        train_start_event.record()  # Start recording GPU time\n","\n","        with tqdm(enumerate(zip(train_articles, train_summaries)), total=len(train_articles), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n","            for idx, (article, summary) in progress:\n","                input_ids = torch.tensor(article).to(device)\n","                labels = torch.tensor(summary).to(device)\n","\n","                outputs = model(input_ids, prompt_id)\n","                logits = outputs.logits\n","\n","                # Calculate loss using predefined criterion\n","                loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","                train_loss += loss.item()\n","\n","                # Backpropagation\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            avg_train_loss = train_loss / len(train_articles)\n","            print(f\"Train Loss (Epoch {epoch + 1}): {avg_train_loss:.4f}\")\n","\n","        train_end_event.record()  # End recording GPU time\n","        train_end_event.synchronize()  # Wait for the events to finish\n","        train_gpu_time = train_start_event.elapsed_time(train_end_event)  # Time in milliseconds\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_predictions = []\n","        val_references = []\n","\n","        # GPU time measurement for validation\n","        val_start_event = torch.cuda.Event(enable_timing=True)\n","        val_end_event = torch.cuda.Event(enable_timing=True)\n","        val_start_event.record()  # Start recording GPU time\n","\n","        with torch.no_grad():\n","            for article, summary in tqdm(zip(val_articles, val_summaries), total=len(val_articles), desc=\"Validation\", unit=\"batch\"):\n","                input_ids = torch.tensor(article).to(device)\n","                labels = torch.tensor(summary).to(device)\n","\n","                outputs = model(input_ids, prompt_id)\n","\n","                # Calculate validation loss\n","                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n","                loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n","                val_loss += loss.item()\n","\n","                # Decode predictions and add to ROUGE computation lists\n","                predicted_token_ids = torch.argmax(outputs.logits, dim=-1)\n","                pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","                ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","                val_predictions.append(pred_text)\n","                val_references.append(ref_text)\n","\n","            avg_val_loss = val_loss / len(val_articles)\n","            avg_rouge1, avg_rouge2, avg_rougeL = calculate_rouge(val_predictions, val_references)\n","            print(f\"Val Loss (Epoch {epoch + 1}): {avg_val_loss:.4f}\")\n","            print(f\"Val ROUGE-1: {avg_rouge1:.4f}, Val ROUGE-2: {avg_rouge2:.4f}, Val ROUGE-L: {avg_rougeL:.4f}\")\n","\n","        val_end_event.record()  # End recording GPU time\n","        val_end_event.synchronize()  # Wait for the events to finish\n","        val_gpu_time = val_start_event.elapsed_time(val_end_event)  # Time in milliseconds\n","\n","        # Early stopping\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            no_improvement_epochs = 0\n","        else:\n","            no_improvement_epochs += 1\n","            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n","                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n","                break\n","\n","        # Print GPU compute time for the epoch\n","        print(f\"GPU Compute Time (Train Epoch {epoch + 1}): {train_gpu_time:.2f} ms\")\n","        print(f\"GPU Compute Time (Validation Epoch {epoch + 1}): {val_gpu_time:.2f} ms\")\n","        \n","        torch.save(model.state_dict(), f\"prompt_tuning_{epoch}.pth\")\n","\n","    total_end_time = time.time()  # End measuring total training time\n","    total_training_time = total_end_time - total_start_time\n","    print(f\"Total training time: {total_training_time:.2f} seconds\")\n","\n","    return model\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T06:52:12.695086Z","iopub.status.busy":"2024-10-29T06:52:12.694484Z","iopub.status.idle":"2024-10-29T06:52:12.699985Z","shell.execute_reply":"2024-10-29T06:52:12.698932Z","shell.execute_reply.started":"2024-10-29T06:52:12.695041Z"},"trusted":true},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1: 100%|██████████| 21000/21000 [1:36:58<00:00,  3.61batch/s]\n","Train Loss (Epoch 1): 9.4259\n","Validation: 100%|██████████| 1337/1337 [02:30<00:00,  8.87batch/s]\n","Val Loss (Epoch 1): 8.8849\n","Val ROUGE-1: 0.0912, Val ROUGE-2: 0.0229, Val ROUGE-L: 0.0579\n","GPU Compute Time (Train Epoch 1): 5819270.50 ms\n","GPU Compute Time (Validation Epoch 1): 195395.25 ms\n","Total training time: 5818 seconds\n"]}],"source":["fine_tuned_model = fine_tune_on_summarization(model, tokenized_articles_train, tokenized_summaries_train, tokenized_articles_validation, tokenized_summaries_validation)\n","\n","torch.save(fine_tuned_model.state_dict(), 'prompt_tuning.pth')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:13:10.655987Z","iopub.status.busy":"2024-10-29T10:13:10.655173Z","iopub.status.idle":"2024-10-29T10:15:54.113823Z","shell.execute_reply":"2024-10-29T10:15:54.112810Z","shell.execute_reply.started":"2024-10-29T10:13:10.655944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing: 100%|██████████| 1149/1149 [02:07<00:00,  9.01batch/s]\n","Test ROUGE-1: 0.0891, Test ROUGE-2: 0.0221, Test ROUGE-L: 0.0566\n"]}],"source":["fine_tuned_model.eval()\n","test_loss=0.0\n","test_predictions = []\n","test_references = []\n","\n","with torch.no_grad():\n","    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Testing\", unit=\"batch\"):\n","        input_ids = torch.tensor(article).to(device)\n","        labels = torch.tensor(summary).to(device)\n","        outputs = fine_tuned_model(input_ids, prompt_id)\n","\n","        # Calculate loss manually\n","        ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n","        loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n","        test_loss += loss.item()\n","\n","        predicted_token_ids = torch.argmax(outputs.logits, dim=-1)\n","        pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","        ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","        test_predictions.append(pred_text)\n","        test_references.append(ref_text)\n","\n","    # Calculate test ROUGE scores\n","    avg_rouge1_test, avg_rouge2_test, avg_rougeL_test = calculate_rouge(test_predictions, test_references)\n","    print(f\"Test ROUGE-1: {avg_rouge1_test:.4f}, Test ROUGE-2: {avg_rouge2_test:.4f}, Test ROUGE-L: {avg_rougeL_test:.4f}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T10:55:00.822791Z","iopub.status.busy":"2024-10-29T10:55:00.822139Z","iopub.status.idle":"2024-10-29T10:55:04.429853Z","shell.execute_reply":"2024-10-29T10:55:04.428931Z","shell.execute_reply.started":"2024-10-29T10:55:00.822750Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/2460819413.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/kaggle/input/prompttuning/pytorch/default/1/prompt_tuning_0.pth'))\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize a new instance of the model\n","model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n","\n","# Load the saved model state_dict\n","model.load_state_dict(torch.load('prompt_tuning.pth'))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-29T11:01:03.778085Z","iopub.status.busy":"2024-10-29T11:01:03.777434Z","iopub.status.idle":"2024-10-29T11:03:49.352534Z","shell.execute_reply":"2024-10-29T11:03:49.351482Z","shell.execute_reply.started":"2024-10-29T11:01:03.778035Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 1149/1149 [02:07<00:00,  9.01batch/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test ROUGE-1: 0.0891, Test ROUGE-2: 0.0221, Test ROUGE-L: 0.0566\n"]}],"source":["model.eval()\n","test_loss = 0\n","test_predictions = []\n","test_references = []\n","test_start_event = torch.cuda.Event(enable_timing=True)\n","test_end_event = torch.cuda.Event(enable_timing=True)\n","test_start_event.record()\n","\n","with torch.no_grad():\n","    for article, summary in tqdm(zip(tokenized_articles_test, tokenized_summaries_test), total=len(tokenized_articles_test), desc=\"Testing\", unit=\"batch\"):\n","        input_ids = torch.tensor(article).to(device)\n","        labels = torch.tensor(summary).to(device)\n","        outputs = model(input_ids, prompt_id)\n","\n","        # Calculate loss manually\n","        ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n","        loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n","        test_loss += loss.item()\n","\n","        predicted_token_ids = torch.argmax(outputs.logits, dim=-1)\n","        pred_text = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n","        ref_text = tokenizer.decode(labels, skip_special_tokens=True)\n","        test_predictions.append(pred_text)\n","        test_references.append(ref_text)\n","\n","    # Calculate test ROUGE scores\n","    avg_rouge1_test, avg_rouge2_test, avg_rougeL_test = calculate_rouge(test_predictions, test_references)\n","    print(f\"Test ROUGE-1: {avg_rouge1_test:.4f}, Test ROUGE-2: {avg_rouge2_test:.4f}, Test ROUGE-L: {avg_rougeL_test:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1654566,"sourceId":2734496,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":150402,"modelInstanceId":127464,"sourceId":150120,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
